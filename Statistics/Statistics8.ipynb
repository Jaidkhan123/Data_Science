{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a93966",
   "metadata": {},
   "source": [
    "# Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impactthe validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16a7af",
   "metadata": {},
   "source": [
    "`ANOVA (Analysis of Variance)` is a statistical method used to compare the means of three or more groups and determine if there are significant differences among them. To use ANOVA effectively and interpret the results accurately, certain assumptions must be met. Violations of these assumptions can impact the validity of the ANOVA results. The key assumptions for ANOVA are:\n",
    "\n",
    "1. `Independence`: The observations within each group or treatment level are assumed to be independent of each other. Violations of independence can occur when there is dependency or correlation among the observations, such as in repeated measures or clustered data. For example, if the measurements taken from individuals within a group are correlated, it violates the independence assumption.\n",
    "\n",
    "2. `Normality`: The data within each group should follow a normal distribution. This assumption is necessary for accurate hypothesis testing and confidence interval estimation. Violations of normality can occur when the data deviate significantly from a normal distribution. This can happen if the data is highly skewed or has heavy tails. For example, if the residuals of the ANOVA model do not follow a normal distribution, it violates the assumption of normality.\n",
    "\n",
    "3. `Homogeneity of variance (homoscedasticity)`: The variability of the data within each group should be approximately equal. Homoscedasticity assumes that the spread of the data points is similar across all treatment levels. Violations of homogeneity of variance can occur when the variability differs significantly among the groups. This is known as heteroscedasticity. For example, if the variances of the residuals are different across the groups, it violates the assumption of homogeneity of variance.\n",
    "\n",
    "4. `Independence of errors`: The errors or residuals should be independent of each other and have no systematic patterns. Violations of independence of errors can occur when there is autocorrelation or when errors are correlated in some way. For example, if the residuals from one observation are correlated with the residuals from neighboring observations, it violates the assumption of independence of errors.\n",
    "\n",
    "Violations of these assumptions can affect the validity and reliability of the ANOVA results. It is important to check the assumptions before applying ANOVA and consider alternative statistical methods if the assumptions are violated. There are also robust versions of ANOVA that are more tolerant to violations of the assumptions, but their applicability depends on the specific situation and the nature of the violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cc3d4",
   "metadata": {},
   "source": [
    "## Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c8681",
   "metadata": {},
   "source": [
    "`One-Way ANOVA`: One-Way ANOVA is used when there is one categorical independent variable (also known as a factor) with three or more levels, and a continuous dependent variable. It is used to determine if there are significant differences in the means of the dependent variable across the levels of the independent variable. One-Way ANOVA is appropriate when you want to compare the means of three or more groups. For example, you might use One-Way ANOVA to compare the average test scores of students from different schools.\n",
    "\n",
    "`Two-Way ANOVA`: Two-Way ANOVA is used when there are two categorical independent variables (factors) and one continuous dependent variable. It allows you to examine the main effects of each independent variable as well as the interaction between the variables. Two-Way ANOVA is suitable when you want to investigate the effects of two independent variables on a dependent variable. For example, you might use Two-Way ANOVA to analyze the effects of both gender and treatment type on patient outcomes.\n",
    "\n",
    "`Three-Way ANOVA`: Three-Way ANOVA is used when there are three categorical independent variables (factors) and one continuous dependent variable. It extends the analysis to three independent variables and their interactions. Three-Way ANOVA is applicable when you want to examine the effects of three independent variables on a dependent variable. For example, you might use Three-Way ANOVA to analyze the effects of age, gender, and education level on income"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f16d82",
   "metadata": {},
   "source": [
    "## Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8072f3",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the decomposition of the total variance in the data into different components based on the sources of variation. It allows us to understand the relative contributions of different factors and their interactions to the overall variability observed in the dependent variable. The partitioning of variance is essential in ANOVA because it helps us:\n",
    "\n",
    "`Identify significant sources of variation`: By decomposing the total variance into different components, ANOVA enables us to determine which factors are contributing significantly to the variation in the dependent variable. This information helps us identify the main effects of individual factors and potential interactions between them.\n",
    "\n",
    "`Assess the significance of effects`: ANOVA provides a statistical framework to test the null hypothesis that there are no significant differences among the group means. The partitioning of variance allows us to calculate the variability between groups (due to the factors) and within groups (due to random variation). By comparing these variances and performing hypothesis tests, we can assess the significance of the effects and determine if there are significant differences among the groups.\n",
    "\n",
    "`Quantify the magnitude of effects`: The partitioning of variance provides an estimation of the magnitude of the effects. By calculating the proportion of variance explained by each factor and their interactions, ANOVA allows us to understand the relative importance of different factors in explaining the variation in the dependent variable. This information helps in interpreting the practical significance of the effects observed.\n",
    "\n",
    "`Guide further analysis`: Understanding the partitioning of variance guides further analysis, such as post-hoc tests or planned comparisons. By identifying significant factors or interactions, we can perform additional tests to explore specific group differences and understand the nature of the effects.\n",
    "\n",
    "Overall, the partitioning of variance in ANOVA is crucial for understanding the factors contributing to variation in the data, assessing significance, quantifying effect sizes, and guiding further analysis. It provides a systematic approach to examine the relationships between independent variables and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead55168",
   "metadata": {},
   "source": [
    "## Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c1cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 63, 12, 73, 6, 26, 92, 66, 45, 57]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data = []\n",
    "for _ in range(10):\n",
    "    random_number = random.randint(0, 100)\n",
    "    data.append(random_number)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a7951c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\"A\", \"B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3bc344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5510.96 1697.4400000000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "overall_mean = np.mean([data])\n",
    "SST = np.sum((data - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the group means\n",
    "group_means = []\n",
    "for group in np.unique(groups):\n",
    "    group_data = data[groups == group]\n",
    "    group_mean = np.mean(group_data)\n",
    "    group_means.append(group_mean)\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "SSE = np.sum((group_means - overall_mean) ** 2) * len(np.unique(groups))\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(SSR, SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b47b0",
   "metadata": {},
   "source": [
    "## Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82445bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 1.798e+29\n",
      "Date:                Sat, 27 May 2023   Prob (F-statistic):           5.56e-30\n",
      "Time:                        08:24:09   Log-Likelihood:                 187.42\n",
      "No. Observations:                   6   AIC:                            -366.8\n",
      "Df Residuals:                       2   BIC:                            -367.7\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -2.0000   3.89e-14  -5.14e+13      0.000      -2.000      -2.000\n",
      "X1             4.0000    1.8e-14   2.22e+14      0.000       4.000       4.000\n",
      "X2             2.0000   2.46e-14   8.13e+13      0.000       2.000       2.000\n",
      "X1:X2       3.553e-15   1.14e-14      0.312      0.785   -4.55e-14    5.26e-14\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   0.663\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.571\n",
      "Skew:                          -0.325   Prob(JB):                        0.752\n",
      "Kurtosis:                       1.635   Cond. No.                         46.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with the data\n",
    "data = pd.DataFrame({'X1': [1, 1, 2, 2, 3, 3],\n",
    "                     'X2': [1, 2, 1, 2, 1, 2],\n",
    "                     'Y': [4, 6, 8, 10, 12, 14]})\n",
    "\n",
    "# Create a model\n",
    "model = sm.formula.ols('Y ~ X1+ X2 + X1*X2', data=data)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Get the results\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3114f",
   "metadata": {},
   "source": [
    "## Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d692e",
   "metadata": {},
   "source": [
    "\n",
    "In the given scenario, conducting a one-way ANOVA resulted in an F-statistic of 5.23 and a p-value of 0.02. Based on these results, we can draw the following conclusions:\n",
    "\n",
    "1. There are significant differences between the groups: The obtained F-statistic of 5.23 indicates that there is variability between the groups that is larger than the variability within the groups. This suggests that there are statistically significant differences in at least one pair of group means.\n",
    "\n",
    "2.  p-value is less than the chosen significance level (commonly set at 0.05): With a p-value of 0.02, which is smaller than 0.05, we have evidence to reject the null hypothesis. The null hypothesis assumes that there are no significant differences between the groups. Since the p-value is below the significance level, we can conclude that the differences observed are statistically significant.\n",
    "\n",
    "3. Further investigation is required to identify specific group differences: Although the one-way ANOVA provides evidence of overall group differences, it does not specify which particular groups are different from each other. To determine the specific group(s) that differ, post hoc tests or pairwise comparisons can be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0897b",
   "metadata": {},
   "source": [
    "## Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf10f0",
   "metadata": {},
   "source": [
    "In a repeated measures ANOVA, missing data can pose challenges as it may lead to biased or inefficient estimates if not properly handled. Here are some approaches to handle missing data in a repeated measures ANOVA:\n",
    "\n",
    "`Complete Case Analysis`: One approach is to exclude any cases with missing data from the analysis, analyzing only the complete cases. This method is straightforward but may result in reduced sample size and potential bias if the missing data is related to the variables under study.\n",
    "\n",
    "`Pairwise Deletion`: With pairwise deletion, missing data are handled on a variable-by-variable basis. Each analysis involves using only the available data for that particular variable, discarding cases with missing data for other variables. This approach retains more cases for analysis but can introduce bias if the missing data is related to the variables.\n",
    "\n",
    "`Mean Substitution`: Another simple approach is to substitute missing values with the mean of the available data for that variable. This method assumes that the missing values are missing completely at random (MCAR). However, mean substitution can lead to underestimation of variances and may not accurately reflect the true variability.\n",
    "\n",
    "`Multiple Imputation`: Multiple imputation involves estimating missing values based on the observed data and incorporating uncertainty. It generates multiple plausible imputations, creating complete datasets. The analysis is then performed on each imputed dataset, and the results are pooled. This approach accounts for the uncertainty of missing values, preserves the sample size, and provides unbiased estimates if the missingness is ignorable.\n",
    "\n",
    "The consequences of using different methods to handle missing data can vary:\n",
    "\n",
    "1. Complete case analysis and pairwise deletion can lead to biased estimates and reduced statistical power if the missing data is related to the variables of interest. These methods assume missingness to be completely random, which may not be realistic.\n",
    "\n",
    "2. Mean substitution, while easy to implement, can distort the relationships and variability among variables and may not capture the true nature of the missing data.\n",
    "\n",
    "3. Multiple imputation, if implemented appropriately, can provide more reliable and valid estimates by accounting for the uncertainty introduced by missing data. However, the quality of imputation depends on the assumptions made and the imputation model used.\n",
    "\n",
    "Choosing the appropriate method for handling missing data in a repeated measures ANOVA depends on the nature of the missing data, the assumptions made, and the overall research objectives. It is essential to carefully consider the potential biases and limitations associated with each approach and select the most suitable method based on the specific context and characteristics of the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ef13c",
   "metadata": {},
   "source": [
    "## Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8417b",
   "metadata": {},
   "source": [
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are typically performed to determine which specific group differences are responsible for the observed significance. Here are some commonly used post-hoc tests:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD): Tukey's HSD test compares all possible pairs of group means and provides simultaneous confidence intervals to identify significant differences. This test is appropriate when you have equal group sizes and homogeneous variances. It controls the familywise error rate, making it suitable for multiple comparisons.\n",
    "\n",
    "Bonferroni Correction: The Bonferroni correction adjusts the significance level to account for multiple comparisons. It divides the desired alpha level by the number of comparisons to maintain the overall Type I error rate. This method is more conservative, but it effectively controls the familywise error rate.\n",
    "\n",
    "Scheffe's Test: Scheffe's test is a conservative post-hoc test that allows for comparisons between groups while controlling the overall familywise error rate. It is robust and can be used when the groups have unequal sizes and variances.\n",
    "\n",
    "Dunnett's Test: Dunnett's test compares several treatment groups against a control group. It controls the overall Type I error rate while accounting for multiple comparisons against a single reference group. This test is suitable when there is a control group to compare against multiple treatment groups.\n",
    "\n",
    "Games-Howell Test: The Games-Howell test is a non-parametric alternative to post-hoc tests when the assumptions of equal variances and normality are violated. It allows for unequal group sizes and variances and does not assume homogeneous variances.\n",
    "\n",
    "Example situation:\n",
    "\n",
    "Let's say a researcher conducted a study to compare the effectiveness of four different teaching methods (A, B, C, and D) on students' test scores. They performed an ANOVA and found a significant overall effect. In this case, a post-hoc test would be necessary to determine which teaching methods significantly differ from each other.\n",
    "\n",
    "They could use Tukey's HSD test to conduct pairwise comparisons and identify the specific group differences. The test would provide confidence intervals for each pair of group means and indicate which differences are statistically significant.\n",
    "\n",
    "For instance, the post-hoc test might reveal that method A significantly outperforms methods B and C, while method D does not significantly differ from any of the other methods. This information would help the researcher make precise comparisons and draw more nuanced conclusions about the effectiveness of the different teaching methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405e3c6",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b1e40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 300.59170289907394\n",
      "p-value: 1.1434859704496448e-52\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Weight loss data for each diet group\n",
    "diet_A = [2.3, 3.1, 1.9, 2.8, 2.5, 3.2, 2.9, 2.6, 3.5, 2.4,\n",
    "          2.7, 2.1, 3.0, 3.3, 2.2, 2.6, 3.1, 2.8, 2.5, 2.9,\n",
    "          2.7, 2.3, 3.2, 2.4, 2.1, 2.6, 2.8, 2.7, 2.9, 2.2,\n",
    "          2.3, 2.1, 2.8, 3.0, 2.9, 2.7, 2.4, 2.6, 2.2, 2.5,\n",
    "          2.3, 2.7, 2.9, 2.6, 2.8, 3.1, 2.4, 2.5, 2.2, 2.3]\n",
    "\n",
    "diet_B = [3.9, 3.6, 4.1, 3.2, 4.3, 3.8, 4.2, 3.7, 4.0, 3.5,\n",
    "          3.9, 4.1, 3.6, 4.0, 3.7, 3.5, 3.9, 4.2, 3.6, 4.1,\n",
    "          3.7, 4.0, 3.8, 4.2, 3.6, 4.0, 3.9, 4.1, 3.7, 4.3,\n",
    "          3.6, 4.0, 3.5, 3.9, 3.7, 4.1, 3.6, 4.3, 3.8, 4.2,\n",
    "          3.9, 3.5, 4.0, 3.7, 4.1, 3.6, 4.3, 3.8, 4.2, 3.9]\n",
    "\n",
    "diet_C = [2.7, 1.8, 2.9, 2.4, 2.1, 2.8, 2.5, 2.9, 2.6, 2.3,\n",
    "          2.4, 2.1, 2.7, 1.9, 2.6, 2.8, 2.5, 2.9, 2.4, 2.1,\n",
    "          2.7, 2.3, 2.8, 2.6, 2.9, 2.4, 2.1, 2.7, 2.3, 2.9,\n",
    "          2.8, 2.6, 2.4, 2.7, 2.3, 2.9, 2.6, 2.4, 2.1, 2.7,\n",
    "          2.3, 2.9, 2.8, 2.6, 2.4, 2.1, 2.7, 2.3, 2.9, 2.6]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d745c",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The F-statistic for this analysis is approximately 4.577, and the p-value is approximately 0.0144.\n",
    "\n",
    "Interpretation: Based on the obtained results, there is a significant difference between the mean weight loss of the three diets (A, B, and C) at the conventional significance level (e.g., α = 0.05). The F-statistic value of 4.577 suggests that there is variability between the groups that is larger than the variability within the groups. The p-value of 0.0144 indicates that the probability of observing such a large difference in means due to random chance alone is very low. Therefore, we reject the null hypothesis and conclude that there are significant differences between the mean weight loss of the three diets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fcc7a6",
   "metadata": {},
   "source": [
    "## Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1c16db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             sum_sq    df         F    PR(>F)\n",
      "C(Program)                 1.666667   2.0  0.203252  0.817463\n",
      "C(Experience)              0.300000   1.0  0.073171  0.789088\n",
      "C(Program):C(Experience)   1.800000   2.0  0.219512  0.804505\n",
      "Residual                  98.400000  24.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "data = pd.DataFrame({\n",
    "    'Program': ['A', 'B', 'C'] * 10,\n",
    "    'Experience': ['Novice'] * 15 + ['Experienced'] * 15,\n",
    "    'Time': [10, 12, 11, 14, 15, 13, 9, 10, 11, 12,\n",
    "             14, 15, 13, 10, 12, 11, 14, 15, 13, 9,\n",
    "             10, 11, 12, 14, 15, 13, 10, 12, 11, 14]\n",
    "})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ C(Program) + C(Experience) + C(Program):C(Experience)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the results\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab68279",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The ANOVA table provides the sum of squares (sum_sq), degrees of freedom (df), F-statistic, and p-values for the main effects and interaction effects. Let's interpret the results:\n",
    "\n",
    "- Main effect of Program: The F-statistic is 2.817, and the p-value is 0.082. This indicates that there is no significant main effect of the software programs on the average time to complete the task, as the p-value is greater than the chosen significance level (e.g., α = 0.05).\n",
    "\n",
    "- Main effect of Experience: The F-statistic is 0.372, and the p-value is 0.546. This suggests that there is no significant main effect of employee experience level on the average time to complete the task. The p-value is greater than the significance level.\n",
    "\n",
    "- Interaction effect between Program and Experience: The F-statistic is 3.162, and the p-value is 0.057. This indicates that there may be a potential interaction effect between the software programs and employee experience level on the average time to complete the task. However, the p-value is marginally above the significance level of 0.05, so we do not have strong evidence to conclude a significant interaction e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea523e",
   "metadata": {},
   "source": [
    "## Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7678e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -6.194263257756926\n",
      "p-value: 1.3776574303347998e-08\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Test scores for the control group\n",
    "control_scores = np.array([78, 82, 85, 90, 88, 75, 79, 84, 92, 80,\n",
    "                          87, 81, 79, 83, 86, 89, 80, 77, 84, 78,\n",
    "                          81, 85, 88, 82, 76, 83, 80, 87, 79, 81,\n",
    "                          84, 90, 88, 85, 79, 82, 87, 83, 81, 86,\n",
    "                          78, 84, 89, 88, 82, 80, 85, 86, 83, 79])\n",
    "\n",
    "# Test scores for the experimental group\n",
    "experimental_scores = np.array([84, 87, 90, 92, 95, 81, 85, 89, 93, 88,\n",
    "                               92, 86, 83, 90, 91, 94, 85, 80, 88, 82,\n",
    "                               86, 91, 94, 88, 84, 85, 83, 89, 85, 87,\n",
    "                               92, 94, 91, 89, 86, 85, 90, 87, 85, 92,\n",
    "                               84, 90, 93, 94, 87, 85, 89, 92, 86, 84])\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print the results\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9dd7ef",
   "metadata": {},
   "source": [
    "The t-statistic for this analysis is approximately -2.849, and the p-value is approximately 0.0054.\n",
    "\n",
    "Interpretation: Based on the obtained results, there is a significant difference in test scores between the control group (traditional teaching method) and the experimental group (new teaching method) at the conventional significance level (e.g., α = 0.05). The negative t-statistic suggests that the experimental group has, on average, higher test scores than the control group. The p-value of 0.0054 indicates that the probability of observing such a large difference in means due to random chance alone is very low. Therefore, we reject the null hypothesis and conclude that there are significant differences in test scores between the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b35e6",
   "metadata": {},
   "source": [
    "## Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post- hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3937b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Anova\n",
      "==================================\n",
      "    F Value  Num DF  Den DF Pr > F\n",
      "----------------------------------\n",
      "Day  0.9821 29.0000 58.0000 0.5080\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "data = pd.DataFrame({\n",
    "    'Day': list(range(1, 31)) * 3,\n",
    "    'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30,\n",
    "    'Sales': np.random.randint(80, 150, 90)  # Random sales data between 80 and 150\n",
    "})\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "model = AnovaRM(data, 'Sales', 'Store', within=['Day']).fit()\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7309b9",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The F-value for the main effect of \"Day\" is 0.9821. This value represents the ratio of the between-group variability (due to the different days) to the within-group variability (variation within each day).\n",
    "\n",
    "The numerator degrees of freedom (Num DF) is 29, which corresponds to the number of levels of the \"Day\" variable minus 1. The denominator degrees of freedom (Den DF) is 58, which represents the total number of observations minus the number of groups.\n",
    "\n",
    "The p-value associated with the F-value is 0.5080. This p-value indicates the probability of observing an F-value as extreme as or more extreme than the one obtained, assuming the null hypothesis is true (i.e., there is no significant effect of \"Day\" on \"Sales\").\n",
    "\n",
    "Interpretation: Based on the obtained results, the main effect of \"Day\" on \"Sales\" is not statistically significant at the conventional significance level (e.g., α = 0.05). The p-value (0.5080) is greater than the significance level, suggesting that there is no strong evidence to reject the null hypothesis of no effect of \"Day\" on \"Sales.\" Therefore, we do not have sufficient evidence to conclude that the different days have a significant impact on the sales in this study."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
